# -*- coding: utf-8 -*-
"""31940757_ass3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nMKMNLB-lz-5EkgvnCZyyYtSiht9KNJy

# **Data Wrangling**


> Assessment-3



> **Pragy Parashar** - 31940757

# **Connecting to Drive**
"""

# connect to google drive
from google.colab import drive
drive.mount('/content/drive')

"""# **Installing and Importing Libraries**"""

pip install PyPDF2

pip install basemap

pip install geopandas

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy
import matplotlib
import geopandas as gpd 
from shapely.geometry import Point, Polygon
# %matplotlib inline
import shapefile
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import Polygon
from matplotlib.collections import PatchCollection
from urllib.request import urlopen
from urllib.parse import quote
from bs4 import BeautifulSoup
import re
import time
import PyPDF2
import numpy as np
from scipy import stats
import math as mat
from sklearn import preprocessing # for linear-regression and scaling
import matplotlib.pyplot as plt   # for plotting graph
from sklearn.model_selection import train_test_split       # for test-train split
from sklearn.linear_model import LinearRegression

"""# **Importing XML and JSON Data**
In this section we will import the property data from xml and json data for the properties across melbourne which is then stored as a df which will form our first four columns for the requirement. The data from both the sources are merged together and after that the duplicates are removed from the combined data.
"""

# reads json file and saves as df
df_json = pd.read_json('/content/drive/Shareddrives/FIT5196_S1_2023/Assessment3/student_data/json/31940757.json')
df_json.head()

# reads xml file and saves as df
df_xml = pd.read_xml('/content/drive/MyDrive/Assessment3/InputFiles/31940757.xml')
df_xml.head()

# concat function will merge the data from both the sources
combined_df = pd.concat([df_json, df_xml], ignore_index=True)
combined_df.head()

# count of unique ids in the combined df
unique_id_count = combined_df['property_id'].nunique()

# Print the count of unique values
print(unique_id_count)

# gets duplicate ids after merging the data
duplicate_ids = combined_df[combined_df['property_id'].duplicated()]
duplicate_ids.head()

# removed the duplicates
clean_combined_df = combined_df.drop_duplicates()

duplicate_ids = clean_combined_df[clean_combined_df['property_id'].duplicated()]
duplicate_ids

"""Even after dropping the duplicates in the above step. We can still observe two duplicates at index position 2238 and 2493 which are removed below."""

# removes the additional duplicates
clean_combined_df = clean_combined_df.drop([2238,2493])

# checks for duplicates
duplicate_ids = clean_combined_df[clean_combined_df['property_id'].duplicated()]
duplicate_ids

"""Now that we have removed the duplicates we will reset the index of the df."""

# reset index values
clean_combined_df = clean_combined_df.reset_index()

# creates additional columns in df with defaut values as NA
clean_combined_df["suburb"] = "NA"
clean_combined_df["lga"] = "NA"
clean_combined_df["closest_train_station_id"] = "NA"
clean_combined_df["distance_to_closest_train_station"] = "NA"
clean_combined_df["travel_min_to_MC"] = "NA"
clean_combined_df["direct_journey_flag"] = "NA"
clean_combined_df["number_of_houses"] = "NA"
clean_combined_df["number_of_units"] = "NA"
clean_combined_df["municipality"] = "NA"
clean_combined_df["aus_born_perc"] = "NA"
clean_combined_df["median_income"] = "NA"
clean_combined_df["median_house_price"] = "NA"
clean_combined_df["population"] = "NA"

# prints top 5 rows from df
clean_combined_df.head()

"""# **Importing Suburb Data**
In this section we will import the subrubs details for the properties using the provided "shp" file. Geopandas is used to read the shp file and the data is stored in a df.
"""

# sets file path 
file_path  = "/content/drive/Shareddrives/FIT5196_S1_2023/Assessment3/supplementary_data/Vic_suburb_boundary/VIC_LOCALITY_POLYGON_shp.shp"
# reads shp file and stores data in df
suburb_df = gpd.read_file(file_path) # reading the shape file
suburb_df.head()

"""For finding the suburbs of the properties we iterate through the main df and create a point object using coordinates of the property. Then this point object is checked to be present in the geometry column of the suburb data. If match is found the repective suburb is added to the list which is then used to update the df with the suburb name."""

# creates an empty list to store suburb names
suburb_list = []
# iterate through the combined df
for k,v in clean_combined_df.iterrows():
    lat = v['lat']
    lon = v['lng']
    # iterates through suburb df
    for i,r in suburb_df.iterrows():
      # creates a point object from the coordinates of properties
      point = Point(lon,lat)
      # checka if the coordinates of the property are contained in that of a suburb
      if r["geometry"].contains(point):
        # adds the match to the suburb list
        suburb_list.append(r["VIC_LOCA_2"]) 
# adds the suburb to the respective proeprties in the combined df
clean_combined_df["suburb"] = suburb_list
clean_combined_df.head()

"""# **Webscraping**
In this section we will be getting the values of the columns using webscraping techniques. 
"""

# creates a set of unique suburbs
suburb_set = list(set(suburb_list))
len(suburb_set)

"""We have 167 unique suburbs across Melbourne which means we will have to scrape through 167 websites for data. We are going to batch process the data as 40 urls per iteration. After every successful we have put a sleep time for 2 min in order to wait for url to process the resposne for the next batch. The scraped data is stored in a dictionary which holds the suburb name as the key."""

# creates empty dictionary to store the scraped values
suburb_dict = {}
# iterates through the first 40 suburbs in the list
for suburb in suburb_set[:40]:
  # sets the url for each suburb
  url = "http://house.speakingsame.com/profile.php?q=" + quote(suburb) + "%2C+VIC"
  # browse through the url
  html = urlopen(url)
  # creates a soup object
  bsObj = BeautifulSoup(html, 'html.parser')

  # Extracting number of houses/units
  sales_table = bsObj.find('table', {'cellspacing': '10', 'style': 'font-size:13px'})
  if sales_table is not None:
    # Extract the number of houses and units
    rows = sales_table.find_all('tr')
    houses,units = rows[3].find_all('td')[2].text.strip().split('/')

  # Extracting municipality
  municipality = bsObj.find('a', href=lambda href: href and 'city.php' in href).text

  # Extracting Australian born percentage
  aus_born_percentage = bsObj.find('td', text='Australian Born').find_next('td').text.strip()

  # Extracting median household income
  median_income = bsObj.find('td', text='Weekly income').find_next('td').text.strip()

  # Extracting median house price
  median_house_price = bsObj.find('td', text='House').find_next('td').text.strip().split()[0]

  # Extracting population
  population = bsObj.find('td', text='All People').find_next('td').text.strip()

  suburb_dict[suburb]= (houses, units, municipality, aus_born_percentage,
                    median_income, median_house_price,  population)
print(suburb_dict)
# delays next execution for 120 seconds
time.sleep(600)

# iterates through next 40 batch of urls
for suburb in suburb_set[40:80]:
  url = "http://house.speakingsame.com/profile.php?q=" + quote(suburb) + "%2C+VIC"
  html = urlopen(url)
  bsObj = BeautifulSoup(html, 'html.parser')

  # Extracting number of houses/units
  sales_table = bsObj.find('table', {'cellspacing': '10', 'style': 'font-size:13px'})
  if sales_table is not None:
    # Extract the number of houses and units
    rows = sales_table.find_all('tr')
    houses,units = rows[3].find_all('td')[2].text.strip().split('/')

  # Extracting municipality
  municipality = bsObj.find('a', href=lambda href: href and 'city.php' in href).text

  # Extracting Australian born percentage
  aus_born_percentage = bsObj.find('td', text='Australian Born').find_next('td').text.strip()

  # Extracting median household income
  median_income = bsObj.find('td', text='Weekly income').find_next('td').text.strip()

  # Extracting median house price
  median_house_price = bsObj.find('td', text='House').find_next('td').text.strip().split()[0]

  # Extracting population
  population = bsObj.find('td', text='All People').find_next('td').text.strip()

  suburb_dict[suburb]= (houses, units, municipality, aus_born_percentage,
                      median_income, median_house_price,  population)
print(suburb_dict)
time.sleep(600)

for suburb in suburb_set[80:120]:
  url = "http://house.speakingsame.com/profile.php?q=" + quote(suburb) + "%2C+VIC"
  html = urlopen(url)
  bsObj = BeautifulSoup(html, 'html.parser')
  
  # Extracting number of houses/units
  sales_table = bsObj.find('table', {'cellspacing': '10', 'style': 'font-size:13px'})
  if sales_table is not None:
    # Extract the number of houses and units
    rows = sales_table.find_all('tr')
    houses,units = rows[3].find_all('td')[2].text.strip().split('/')

  # Extracting municipality
  municipality = bsObj.find('a', href=lambda href: href and 'city.php' in href).text

  # Extracting Australian born percentage
  aus_born_percentage = bsObj.find('td', text='Australian Born').find_next('td').text.strip()

  # Extracting median household income
  median_income = bsObj.find('td', text='Weekly income').find_next('td').text.strip()

  # Extracting median house price
  median_house_price = bsObj.find('td', text='House').find_next('td').text.strip().split()[0]

  # Extracting population
  population = bsObj.find('td', text='All People').find_next('td').text.strip()

  suburb_dict[suburb]= (houses, units, municipality, aus_born_percentage,
                      median_income, median_house_price,  population)
print(suburb_dict)
time.sleep(600)

for suburb in suburb_set[120:]:
  url = "http://house.speakingsame.com/profile.php?q=" + quote(suburb) + "%2C+VIC"
  html = urlopen(url)
  bsObj = BeautifulSoup(html, 'html.parser')

  # Extracting number of houses/units
  sales_table = bsObj.find('table', {'cellspacing': '10', 'style': 'font-size:13px'})
  if sales_table is not None:
    # Extract the number of houses and units
    rows = sales_table.find_all('tr')
    houses,units = rows[3].find_all('td')[2].text.strip().split('/')

  # Extracting municipality
  municipality = bsObj.find('a', href=lambda href: href and 'city.php' in href).text

  # Extracting Australian born percentage
  aus_born_percentage = bsObj.find('td', text='Australian Born').find_next('td').text.strip()

  # Extracting median household income
  median_income = bsObj.find('td', text='Weekly income').find_next('td').text.strip()

  # Extracting median house price
  median_house_price = bsObj.find('td', text='House').find_next('td').text.strip().split()[0]

  # Extracting population
  population = bsObj.find('td', text='All People').find_next('td').text.strip()

  suburb_dict[suburb]= (houses, units, municipality, aus_born_percentage,
                    median_income, median_house_price,  population)
print(suburb_dict)

clean_combined_df.head()

# iterates through the dictionary created containing the scraped data
for keys in suburb_dict.keys():
  # iterates through the index of the df
  for idx in list(clean_combined_df.index.values):
    # checks suburb match is found in dictionary
    if str(clean_combined_df.loc[idx, "suburb"]).upper().strip() == keys.upper().strip():
      # adds the scrapes data to the df
      clean_combined_df.at[idx, "number_of_houses"] = int(suburb_dict[keys][0])
      clean_combined_df.at[idx, "number_of_units"] = int(suburb_dict[keys][1])
      clean_combined_df.at[idx, "municipality"] = str(suburb_dict[keys][2]).strip()
      clean_combined_df.at[idx, "aus_born_perc"] = int(re.findall(r'\d+', suburb_dict[keys][3])[0])
      clean_combined_df.at[idx, "median_income"] = int(suburb_dict[keys][4].replace('$', '').replace(',', ''))
      clean_combined_df.at[idx, "median_house_price"] = int(suburb_dict[keys][5].replace('$', '').replace(',', ''))
      clean_combined_df.at[idx, "population"] = int(suburb_dict[keys][6])
clean_combined_df.head()

from os import chdir
chdir("/content/drive/MyDrive/Assessment3")

clean_combined_df.to_csv("combined.csv")

"""# **Importing LGA Data**
LGA data is imported from the pdf containing the area along with the suburbs contained within that area. 
"""

# Create a dictionary
lga_dict = {}
# Open the PDF file in read binary mode
with open('/content/drive/Shareddrives/FIT5196_S1_2023/Assessment3/supplementary_data/Lga_to_suburb.pdf', 'rb') as file:
    # Create a PDF reader object
    pdf_reader = PyPDF2.PdfReader(file)

    # Get the total number of pages in the PDF
    num_pages = len(pdf_reader.pages)
 
    # Read the content of each page
    for page_number in range(num_pages):
        # Get the page object
        page = pdf_reader.pages[page_number]

        # Extract the text from the page
        page_text = page.extract_text()

        # Split the string into separate lines
        lines = page_text.split('\n')
        # Iterate over lines and convert to dictionary
        for line in lines:
          # creates a list to store the suburb names within a lga
          val_list = []
          # splits data in to lga and suburbs
          key, value = line.split(':')
          # removes unwanted string elements from the suburb
          value = re.sub(r"\[", "", value)
          value = re.sub(r"\]", "", value)
          value = re.sub(r"'", "", value)
          # splits the string of suburbs
          value = value.split(",")
          # iterates through the list of suburbs
          for i in value:
            # removes any unwanted space character
            i = re.sub(r'([a-z])\s+([a-z])', r'\1\2', i)
            # adds the suburb to a list
            val_list.append(i.upper().strip())
          # adds the list of suburbs to the designated lga
          lga_dict[key.strip()] = val_list
lga_dict

# iterates main df
for idx in list(clean_combined_df.index.values):
  # iterates lga dictionary
  for keys in lga_dict.keys():
    # checks if a suburb is in the lga
    if str(clean_combined_df.loc[idx, "suburb"]).upper().strip() in lga_dict[keys]:
      # adds the lga to the df
      clean_combined_df.at[idx, "lga"] = keys
clean_combined_df.head()

"""# **Closest Station**
We use haversine function to find the closest train station from a property. We are provided with the property coordinates. The coordinates for all the stations is extracted from the stops.txt file provided.
"""

# reads all the suplementry data files provided.
agency_df = pd.read_csv("/content/drive/Shareddrives/FIT5196_S1_2023/Assessment3/supplementary_data/Vic_GTFS_data/metropolitan/agency.txt")
calender_dates_df = pd.read_csv("/content/drive/Shareddrives/FIT5196_S1_2023/Assessment3/supplementary_data/Vic_GTFS_data/metropolitan/calendar_dates.txt")
calender_df = pd.read_csv("/content/drive/Shareddrives/FIT5196_S1_2023/Assessment3/supplementary_data/Vic_GTFS_data/metropolitan/calendar.txt")
routes_df = pd.read_csv("/content/drive/Shareddrives/FIT5196_S1_2023/Assessment3/supplementary_data/Vic_GTFS_data/metropolitan/routes.txt")
shapes_df = pd.read_csv("/content/drive/Shareddrives/FIT5196_S1_2023/Assessment3/supplementary_data/Vic_GTFS_data/metropolitan/shapes.txt")
stop_times_df = pd.read_csv("/content/drive/Shareddrives/FIT5196_S1_2023/Assessment3/supplementary_data/Vic_GTFS_data/metropolitan/stop_times.txt")
stops_df = pd.read_csv("/content/drive/Shareddrives/FIT5196_S1_2023/Assessment3/supplementary_data/Vic_GTFS_data/metropolitan/stops.txt")
trips_df = pd.read_csv("/content/drive/Shareddrives/FIT5196_S1_2023/Assessment3/supplementary_data/Vic_GTFS_data/metropolitan/trips.txt")

stops_df.head(5)

# function to calculate arc distance between two locations
def calculate_min_distance(lat1,lat2,lon1,lon2):
  '''
  parameters:
    lat1: latitude value of location 1
    lat2: latitude value of location 2
    lon1: longitude value of location 1
    lon2: longitude value of location 2
  returns:
    float value which is the arc distance between the two coordinates
  '''
  # converts the coordinates to radians from degree
  lat1 = mat.radians(lat1)
  lat2 = mat.radians(lat2)
  long1 = mat.radians(lon1)
  long2 = mat.radians(lon2)
  lat = lat1 - lat2
  lon = long1 - long2
  # haversine formula to calculate the distance
  a = mat.sin(lat/2)**2 + mat.cos(lat2) * mat.cos(lat1) * mat.sin(lon/2)**2
  c = 2 * mat.asin(mat.sqrt(a)) 
  # multiply the arc distance with the radius of earth
  dist = 6378 * c
  # return the calculated distance
  return dist

# iterates through the main df
for idx2 in list(clean_combined_df.index.values):
  # initializes a min distance variable
  min_dist = 100
  # variable to hold the closest station id
  closest_station = 0
  # iterates through the stops_df
  for idx1 in list(stops_df.index.values):
    # property lat
    lat1 = clean_combined_df.loc[idx2, "lat"]
    # property long
    lon1 = clean_combined_df.loc[idx2, "lng"]
    # station lat
    lat2 = stops_df.loc[idx1, "stop_lat"]
    # station long
    lon2 = stops_df.loc[idx1, "stop_lon"]
    # calculates distance using the function
    dist = round(calculate_min_distance(lat1, lat2, lon1, lon2),3)
    # checks if the dist is less than the present value of min_dist
    if (min_dist > dist):
      # sets the dist to minimum value
      min_dist = dist
      closest_station = stops_df.loc[idx1, "stop_id"]
      #print(closest_station)
  clean_combined_df.at[idx2, "closest_train_station_id"] =  closest_station
  clean_combined_df.at[idx2, "distance_to_closest_train_station"] = min_dist

clean_combined_df.head()

clean_combined_df.head()

"""# **Direct Trip to Melbourne Central**"""

# getting stop id for Melbourne Central Station
destination_list = stops_df.loc[stops_df.stop_name == "Melbourne Central Railway Station"].stop_id.tolist()
destination_list

"""Here we obtain the list of stop ids for Melbourne Central as a stop."""

# df to get all stop time at Melbourne Central
stop_mcs = stop_times_df[stop_times_df['stop_id'].isin(destination_list)] 
stop_mcs.head()

# filtering the trips between 7 A.M. and 9 A.M.
trip_mcs = stop_mcs.loc[(stop_mcs.departure_time>='07:00:00') & (stop_mcs.departure_time<='09:00:00')]
# list of trips between the required hours
mcs_trip_list = trip_mcs['trip_id'].tolist()

# list of services that run on all weekdays
weekday_service_list =  []
# iterates through calendar df
for idx in list(calender_df.index.values):
    # filter for services running on weekdays
    if calender_df.loc[idx,'monday'] == 1 and calender_df.loc[idx,'tuesday'] ==1 and calender_df.loc[idx,'wednesday'] == 1 and calender_df.loc[idx,'thursday'] ==1 and calender_df.loc[idx,'friday']==1:
      weekday_service_list.append(calender_df.loc[idx, 'service_id'])
weekday_service_list

"""From all the services that run across Melbourne we have T0 running all the weekdays."""

# df with all the services and trips on weekdays
cal_trip_df = pd.merge(trips_df[["service_id","trip_id"]],calender_df[['service_id','monday','tuesday','wednesday','thursday','friday']],
                     on = 'service_id') # merging the two dataframes
cal_trip_df.head()

# merging the stop timings with the cal_trip df
network_df = pd.merge(stop_times_df[["trip_id","stop_id","arrival_time","departure_time","stop_sequence"]],
                      cal_trip_df,on='trip_id')
network_df.head()

# getting weekday services from the network to Melbourne Central
weekday_network = network_df[(network_df['service_id'].isin(weekday_service_list)) & network_df['trip_id'].isin(mcs_trip_list)]
# all morning services between 7 and 9 AM.
morn_services_df = weekday_network[(weekday_network.departure_time>='07:00:00') & (weekday_network.departure_time<='09:30:00')]
# dropping duplicates
morn_services_df.drop_duplicates(inplace = True, keep = 'first',ignore_index=True)
morn_services_df.head()

# trips stopping at Melbourne Central
mcr_stop_df = morn_services_df[morn_services_df['stop_id'].isin(destination_list)]
# list of trip ids to Melbourne central
mcr_trips_list = mcr_stop_df['trip_id'].tolist() 
# list of stop sequences to Melbourne Central
stop_seq_list = mcr_stop_df['stop_sequence'].tolist() 
# dictionary for trips and stop sequence to Melb Central
mcr_trips_dict = dict(zip(mcr_trips_list, stop_seq_list))

# removing stop sequences greater than of Melb Central
morn_services_df.reset_index(drop=True, inplace=True)
temp_services_df = morn_services_df.copy()
for idx in list(temp_services_df.index.values):
  try:
    trip_id  = temp_services_df.loc[idx,'trip_id']
    seq = mcr_trips_dict[trip_id]
    morn_services_df.drop(temp_services_df[(temp_services_df['trip_id']== trip_id) 
                                    & (temp_services_df['stop_sequence']>seq)].index,inplace =True)
  except:
    pass
morn_services_df.head()

"""Removing these stop sequences ensures that the trains are going towards Melbourne Central Station."""

stop_id_list = morn_services_df['stop_id'].tolist()
# removing dupicates and storing unique stop IDs
u_stop_id_list = list(set(stop_id_list))
len(u_stop_id_list)

clean_combined_df

for idx in list(clean_combined_df.index.values):
    if clean_combined_df.loc[idx, "closest_train_station_id"] in u_stop_id_list:
      clean_combined_df.at[idx, "direct_journey_flag"] = '1'
    else:
      clean_combined_df.at[idx, "direct_journey_flag"] ='0'
clean_combined_df.head()

jourey_df = morn_services_df[['trip_id',"stop_id",	"arrival_time",
                              "departure_time"]]
jourey_df.head()

# list of station id's
closest_station_list = clean_combined_df["closest_train_station_id"].tolist()
# list of trips for the station ids
closest_trip_id_list = jourey_df[jourey_df["stop_id"].isin(closest_station_list)].trip_id.tolist()

# df for the closest stations as a source
start_journ_df = jourey_df[jourey_df["trip_id"].isin(closest_trip_id_list)]
# df for Mlebourne Central as destination
end_journ_df = jourey_df[jourey_df["trip_id"].isin(mcr_trips_list)]

end_journ_df

time_calc_df = pd.merge(start_journ_df,end_journ_df,on="trip_id")
# drop extra columns
time_calc_df.drop("arrival_time_x",axis=1,inplace = True)
time_calc_df.drop("departure_time_y",axis=1,inplace = True)
time_calc_df.rename(columns={'arrival_time_y': 'arrival_time'}, inplace=True)
time_calc_df.rename(columns={'departure_time_x': 'departure_time'}, inplace=True)
time_calc_df.head()

# filtering the dataframe and keeping only those values where departure time is less than arrival time
time_calc_df = time_calc_df[time_calc_df['departure_time'] <= time_calc_df['arrival_time']]
time_calc_df.drop_duplicates(inplace = True, keep = 'first',ignore_index=True)
time_calc_df.head()

# transfrom time to minutes
departure = pd.DatetimeIndex(time_calc_df['departure_time'])
departure_mins = departure.hour * 60 + departure.minute
time_calc_df['departure_time'] = departure_mins

arrivals = pd.DatetimeIndex(time_calc_df['arrival_time'])
arrivals_mins = arrivals.hour * 60 + arrivals.minute
time_calc_df['arrival_time'] = arrivals_mins
time_calc_df.head()

# calculates duration of the journey in 
time_calc_df["journey_time"] = time_calc_df['arrival_time'] - time_calc_df['departure_time']
time_calc_df.head()

journey_avg_df = time_calc_df.groupby('stop_id_x')['journey_time'].mean().reset_index()
journey_avg_df.head()

for idx1 in list(journey_avg_df.index.values):
  for idx2 in list(clean_combined_df.index.values):
    if clean_combined_df.loc[idx2, "closest_train_station_id"] == journey_avg_df.loc[idx1, "stop_id_x"]:
      if clean_combined_df.loc[idx, "direct_journey_flag"] == '1':
        clean_combined_df.at[idx2, "travel_min_to_MC"] = str(journey_avg_df.loc[idx1, "journey_time"])
      else:
        clean_combined_df.at[idx2, "travel_min_to_MC"] = "no direct trip is available"
clean_combined_df.head()

clean_combined_df.head()

chdir("/content/drive/MyDrive/Assessment3")
clean_combined_df.to_csv("31940757_A3_solution.csv", index=False)

"""# **Data Reshaping**"""

reshape_df = clean_combined_df[['number_of_houses','number_of_units','population','aus_born_perc','median_income','median_house_price']]
reshape_df.head()

# graph of values before standardization
reshape_df['number_of_houses'].plot(),reshape_df['number_of_units'].plot(),reshape_df['population'].plot(),reshape_df['aus_born_perc'].plot(),reshape_df['median_income'].plot()

"""From the above plot we can see that the data deoendency is heavily skewed. There is dominating dependency on one of the feature within the data. Therefore, the need of data reshaping comes into place in order have a more uniform and linear relationship between the dependent and independent feature.

**Z-Score Tranformation / Standardization**

Data standardization is the process of converting data values into a uniform scale or distribution. Making sure that all variables are on an equal footing and removing any bias that may result from variations in the original scales or units of measurement are common preprocessing steps in data analysis and machine learning projects.

Standardisation makes it simpler to analyse and compare data across various factors. When variables have diverse measuring scales or units, it is especially helpful because it converts them to a standardised scale. Standardisation helps algorithms converge more quickly during machine learning training by removing scale disparities and prevents some characteristics from predominating over others based only on their original scales.

In the code below we will scale the scarped columns so that they have their mean close to zero and standard deviation close to one. This will cause our features to have a rather equal impact on the dependent variable than some dominant.
"""

# scale the scraped columns using StandardScaler class from the sklearn.preprocessing S
norm_scale = preprocessing.StandardScaler().fit(reshape_df[['number_of_houses','number_of_units','population', 'aus_born_perc','median_income']])
# applies the scaling to the columns and stores in a df
df_norm = norm_scale.transform(reshape_df[['number_of_houses','number_of_units','population', 'aus_born_perc','median_income',]])
# prints the normalized df
df_norm[0:5]

# creates a new df
normal_df = pd.DataFrame()

# adds the scaled values to each column
normal_df['number_of_houses']= df_norm[:,0]
normal_df['number_of_units']= df_norm[:,1]
normal_df['population']= df_norm[:,2]
normal_df['aus_born_perc']= df_norm[:,3]
normal_df['median_income']= df_norm[:,4]

# plotting the scaled values
normal_df['number_of_houses'].plot(),normal_df['number_of_units'].plot(),normal_df['population'].plot(),normal_df['aus_born_perc'].plot(),normal_df['median_income'].plot()

"""**Min-Max Normalization**

The normalisation of data using the Min-Max method is another well-liked method. It adjusts the values so that they correspond to a predetermined range, usually ranging from 0 to 1. The terms feature scaling and rescaling are also used to describe this normalisation method.
By using min-max normalisation, the feature's minimum and maximum values are scaled to 0 and 1, respectively. Within that range, the remaining numbers are correspondingly scaled. In contrast to the Z-transform, we are scaling the values between 0 and 1 than having their mean and deviation close to the respective values.
"""

# sclaing the scraped columns
minmax = preprocessing.MinMaxScaler().fit(reshape_df[['number_of_houses','number_of_units','population', 'aus_born_perc','median_income']]) 
# applies the scaling to the columns and stores in a df
minmax_df = minmax.transform(reshape_df[['number_of_houses','number_of_units','population', 'aus_born_perc','median_income']])
minmax_df[0:5]

# creates a new df
mm_df = pd.DataFrame()
# adds the scaled values to each column
mm_df['number_of_houses_mm'] = minmax_df[:,0]              
mm_df['number_of_units_mm'] = minmax_df[:,1] 
mm_df['population_mm'] = minmax_df[:,2] 
mm_df['aus_born_perc_mm'] = minmax_df[:,3]
mm_df['median_income'] = minmax_df[:,4]  
mm_df.head()

# plotting the scaled values
mm_df['number_of_houses_mm'].plot(), mm_df['number_of_units_mm'].plot(), 
mm_df['population_mm'].plot(), mm_df['aus_born_perc_mm'] .plot(),
mm_df['median_income'] .plot()

"""**Log Transform**

A mathematical technique known as a log transformation is used to determine a variable's logarithm. It is frequently applied in data analysis and modelling to resolve problems like skewed distributions or to reduce a variable's volatility. Either the common logarithm (base 10) or the natural logarithm (base e) can be used as the logarithm for transformation.
In some circumstances, log transformations can help to linearize relationships, make the data more symmetric, and lessen the influence of outliers.
"""

# creates a new df to perform log transform
log_trfm_df = reshape_df.copy()
# iterates through the df
for idx in list(log_trfm_df.index.values):
  # calculates the log values of each cell values for the dependant variable
  log_trfm_df.at[idx, "number_of_houses"] =  mat.log(log_trfm_df.loc[idx, "number_of_houses"])
  log_trfm_df.at[idx, "number_of_units"] =  mat.log(log_trfm_df.loc[idx, "number_of_units"])
  log_trfm_df.at[idx, "population"] =  mat.log(log_trfm_df.loc[idx, "population"])
  log_trfm_df.at[idx, "aus_born_perc"] =  mat.log(log_trfm_df.loc[idx, "aus_born_perc"])
  log_trfm_df.at[idx, "median_income"] =  mat.log(log_trfm_df.loc[idx, "median_income"])
log_trfm_df.head()

# plotting the scaled values
log_trfm_df['number_of_houses'].plot(), log_trfm_df['number_of_units'].plot(), 
log_trfm_df['population'].plot(), log_trfm_df['aus_born_perc'] .plot(),
log_trfm_df['median_income'] .plot()

"""**Power Transform**

Square power transformation is a straightforward data transformation method that squares the values of a variable. It is a non-linear transformation that can be used to examine relationships in the data or solve specific problems.
The square transformation can affect the data in a variety of ways:


> Symmetry: Using a square transformation to the original data can assist make the distribution more symmetric if it has a positive or negative skew.


> Outliers: Squaring the variable's values might increase the impact of outliers. Larger than average outliers will have a more significant effect on the transformed numbers.



> The square transformation can be used to investigate and depict non-linear interactions between variables. It can expose patterns that otherwise wouldn't reveal in the actual data.



"""

power_df = reshape_df.copy()
# iterates through the df
for idx in list(power_df.index.values):
  # calculates the sqaured values of each cell values for the dependant variable
  power_df.at[idx, "number_of_houses"] =  mat.pow(log_trfm_df.loc[idx, "number_of_houses"],2)
  power_df.at[idx, "number_of_units"] =  mat.pow(log_trfm_df.loc[idx, "number_of_units"],2)
  power_df.at[idx, "population"] =  mat.pow(log_trfm_df.loc[idx, "population"],2)
  power_df.at[idx, "aus_born_perc"] =  mat.pow(log_trfm_df.loc[idx, "aus_born_perc"],2)
  power_df.at[idx, "median_income"] =  mat.pow(log_trfm_df.loc[idx, "median_income"],2)
power_df.head()

# plotting the scaled values
power_df['number_of_houses'].plot(), power_df['number_of_units'].plot(), 
power_df['population'].plot(), power_df['aus_born_perc'] .plot(),
power_df['median_income'] .plot()

"""**Box-Cox Transfrom**

By identifying the ideal power parameter (lambda) that maximises the likelihood of the transformed data, the Box-Cox transformation is a well-known power transformation approach that can be used to normalise the distribution of a variable. This transformation works well with variables that are heteroscedastic or non-normal.
The Box-Cox transformation is a versatile method that can aid in normalising a variable's distribution and stabilising its variance, making it more appropriate for specific statistical studies or modelling techniques. The qualities of the data and the desired transformation effect influence the choice of the lambda parameter.
The lambda parameter determines the type of transformation:

> If lambda = 0: It applies a logarithmic transformation to the data.

> If lambda = 1: It applies no transformation to the data.


> If lambda = -1: It applies an inverse transformation to the data.
"""

box_df = reshape_df.copy()

# columns to be transformed
columns_to_transform = ['number_of_houses', 'number_of_units', 'population', 'aus_born_perc', 'median_income']

# change column values to float
box_df[columns_to_transform] = box_df[columns_to_transform].astype(float)

 #ransformation on the columns
transformed_data = box_df[columns_to_transform].apply(lambda x: stats.boxcox(x)[0])

transformed_data.head()

# plotting the scaled values
transformed_data['number_of_houses'].plot(), transformed_data['number_of_units'].plot(), 
transformed_data['population'].plot(), transformed_data['aus_born_perc'] .plot(),
transformed_data['median_income'] .plot()

"""Observing all the plots we can that the data when transformed using box-cox transformation leads to much more linear and unifrom data. Therefore, we should be using this transfromation for predicting the house prices. Also, log scale transfromation provides least linearity within the data, hence making it unsuitable to train a linear model.

# **References**


> Convert time (HH:MM:SS) to minutes in python - https://stackoverflow.com/questions/48447123/convert-time-hhmmss-to-minutes-in-python


> Box-Cox transformation - https://www.geeksforgeeks.org/box-cox-transformation-using-python/


> Power Transform - https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html


> Reading shape files - https://gis.stackexchange.com/questions/113799/reading-shapefile-in-python
"""